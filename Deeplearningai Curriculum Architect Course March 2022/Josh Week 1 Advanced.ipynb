{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Josh Week 1 Advanced.ipynb","provenance":[],"authorship_tag":"ABX9TyNWAUr7x/f3fOA56OkS8jfH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Missing the Forest for the Trees:\n","## Introduction to Slow Feature Analysis\n","\n","### (Re)Sources:\n","\n","- http://www.scholarpedia.org/article/Slow_feature_analysis\n","- https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3076122\n","\n","### Introduction:\n","\n","If you havent heard of slow feature analysis, you are not alone, it is an algorithm and more general method that often gets overshadowed by its cousin Principal Component Analysis, thats right the famous PCA has a cousin, and it turns out it might have something to do with how your brain is processing these very visual stimuli! Since this is a less well known topic this is a tutorial that attempts to concretize what can be an overly abstract method for those who want to look at features that are often lost due to overly noisy data or unnecesarily complex infered solutions. SFA allows us to see the forest AND the trees, to transform a highly nonlinear problem into a nice and approachable linear one.\n","\n","### Target Learner:\n","The Target Learner for this tutorial wants implement (and use) slow feature analysis (SFA) and distinguish the results of the SFA algorithm from Principal Component Analysis (PCA) and other related eigenvalue based methods. \n","\n","### Learning Objectives:\n","By the end of this course the learner will create a basic implmentation of the Slow Feature Analysis algorithm and visualize the results.\n","\n","### Assumptions:\n","We assume the learner is proficient in at least basic linear algebra and statistics (and a teeny tiny bit of differential equations) as well as python, basic numpy, and visualization libraries like Matplotlib. We assume the learner understands decorrelation as well as at least the basic premise (and how to use) PCA. It would be helpful if the learner has seen optimisation problems before treateed mathematically. \n","\n","### Syllabus:\n","\n","- Video: Introduction: What is Slow Feature Analysis 2 Min\n","- Text: Sketch of Algorithm\n","- Coding: Generating Sample Data (maybe I will just have data ready for them, though generating data is always good practice)\n","- Video: Transforming a Nonlinear Problem to a Linear One (Kernal Trick) 3 Min\n","- Coding: Expanding the data into 3rd degree polynomials\n","- Text: What Decorrelation Means Here\n","- Coding: Sphering the expanded data (to satisfy SFA constraints)\n","- Text: A very gentle differential equation\n","- Coding: Calculating the discrete derivative of the expanded data (numpy)\n","- Text: the magic of gaussians makes for clean calculations\n","- Coding: Getting the eigenvalues and Eigenvectors (numpy)\n","- Coding: Selecting the slowest varying features (smallest eigenvalues)\n","- Coding: Visualizing the Results (I think I would just do the skeleton here so the learner doesnt have to bother with it)\n","- Quiz: Differences Betweeen SFA and PCA\n","- Video: Summary of Differences and uses in Neurosciennce 2 Min\n","\n"],"metadata":{"id":"4EVFBpL5gW8M"}}]}